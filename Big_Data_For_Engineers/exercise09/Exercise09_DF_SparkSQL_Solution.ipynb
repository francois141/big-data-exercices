{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Big Data for Engineers &ndash; Exercises &ndash; Solution</center>\n",
    "## <center>Spring 2022 &ndash; Week 9 &ndash; ETH Zurich</center>\n",
    "## <center>Spark Dataframes and SparkSQL</center>\n",
    "\n",
    "# Preparation for the exercise in Spark\n",
    "\n",
    "1. Change to `exercise09` repository\n",
    "\n",
    "2. Start docker <br>\n",
    "```docker-compose up -d``` <br>\n",
    "(This process can take up to 10 minutes.)\n",
    "\n",
    "3. After docker finishes downloading the images, you should be able to start the jupyter notebook by copying the following URL to your browser <br>\n",
    "```http://localhost:8888/lab```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>1. Spark Dataframes</center>\n",
    "\n",
    "Spark Dataframes allow the user to perform simple and efficient operations on data, as long as the data is structured and has a schema. Dataframes are similar to relational tables in relational databases: conceptually a dataframe is a specialization of a Spark RDD with schema information attached. You can find more information in Karau, H. et al. (2015). Learning Spark, Chapter 9 (optional reading).\n",
    "\n",
    "Throughout the exercise, you can see the equivalency of Spark RDD, Spark Dataframes and SparkSQL. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 3314.6728515625,
      "end_time": 1573739117708.542
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "spark = SparkSession.builder.master('local').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "path = \"orders.jsonl\"\n",
    "orders_df = spark.read.json(path).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The type of our dataset object is DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 35.862060546875,
      "end_time": 1573665101742.127
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(orders_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 251.81103515625,
      "end_time": 1573665103317.247
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer: struct (nullable = true)\n",
      " |    |-- first_name: string (nullable = true)\n",
      " |    |-- last_name: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- items: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- price: double (nullable = true)\n",
      " |    |    |-- product: string (nullable = true)\n",
      " |    |    |-- quantity: long (nullable = true)\n",
      " |-- order_id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print one row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 3283.30908203125,
      "end_time": 1573665107643.345
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(customer=Row(first_name='Preston', last_name='Landry'), date='2018-2-4', items=[Row(price=1.53, product='fan', quantity=5), Row(price=1.33, product='computer screen', quantity=6), Row(price=1.06, product='kettle', quantity=6), Row(price=1.96, product='stuffed animal', quantity=3), Row(price=1.09, product='the book', quantity=7), Row(price=1.42, product='headphones', quantity=9), Row(price=1.67, product='whiskey bottle', quantity=3)], order_id=0)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders_df.limit(1).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the underlying RDD object and use any functions you learned for Spark RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 17329.39501953125,
      "end_time": 1573665345486.969
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1960"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders_df.rdd.filter(lambda ordr: ordr.customer.last_name == \"Landry\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Dataframe Operations\n",
    "We perform some queries using operations on Dataframes ([Here](https://spark.apache.org/docs/2.3.0/sql-programming-guide.html#untyped-dataset-operations-aka-dataframe-operations) is a guide on DF Operations with a link to the [API Documentation](https://spark.apache.org/docs/2.3.0/api/python/pyspark.sql.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can select columns and show the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 252.5771484375,
      "end_time": 1573665989686.293
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|first_name|last_name|\n",
      "+----------+---------+\n",
      "|   Preston|   Landry|\n",
      "|    Jamari|Dominguez|\n",
      "|   Brendon|  Sicilia|\n",
      "|    Armani|   Ardeni|\n",
      "|    Jamari|     Miao|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.select(\"customer.first_name\", \"customer.last_name\").limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also convert the dataframe object to a pandas dataframe to display it nicely in jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Preston</td>\n",
       "      <td>Landry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jamari</td>\n",
       "      <td>Dominguez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brendon</td>\n",
       "      <td>Sicilia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Armani</td>\n",
       "      <td>Ardeni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jamari</td>\n",
       "      <td>Miao</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  first_name  last_name\n",
       "0    Preston     Landry\n",
       "1     Jamari  Dominguez\n",
       "2    Brendon    Sicilia\n",
       "3     Armani     Ardeni\n",
       "4     Jamari       Miao"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders_df.select(\"customer.first_name\", \"customer.last_name\").limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we can navigate to the nested items with the dot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 2263.60888671875,
      "end_time": 1573665774856.528
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1960"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders_df.filter(orders_df[\"customer.last_name\"] == \"Landry\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about nested arrays?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 251.12890625,
      "end_time": 1573666229796.764
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|order_id|               items|\n",
      "+--------+--------------------+\n",
      "|       0|[{1.53, fan, 5}, ...|\n",
      "|       1|[{1.61, fan, 7}, ...|\n",
      "|       2|[{1.41, the book,...|\n",
      "|       3|[{1.05, computer ...|\n",
      "|       4|[{1.92, headphone...|\n",
      "+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.select(\"order_id\", \"items\").orderBy(\"order_id\").limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to find orders of a fan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 239.119140625,
      "end_time": 1573666737735.271
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[DATATYPE_MISMATCH.BINARY_OP_DIFF_TYPES] Cannot resolve \"(items.product = fan)\" due to data type mismatch: the left and right operands of the binary operator have incompatible types (\"ARRAY<STRING>\" and \"STRING\").;\n'Filter (items#10.product = fan)\n+- Relation [customer#8,date#9,items#10,order_id#11L] json\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43morders_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43morders_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mitems.product\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcount()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:3138\u001b[0m, in \u001b[0;36mDataFrame.filter\u001b[0;34m(self, condition)\u001b[0m\n\u001b[1;32m   3136\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mfilter(condition)\n\u001b[1;32m   3137\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(condition, Column):\n\u001b[0;32m-> 3138\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcondition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3139\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   3141\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_COLUMN_OR_STR\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3142\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcondition\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(condition)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m   3143\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [DATATYPE_MISMATCH.BINARY_OP_DIFF_TYPES] Cannot resolve \"(items.product = fan)\" due to data type mismatch: the left and right operands of the binary operator have incompatible types (\"ARRAY<STRING>\" and \"STRING\").;\n'Filter (items#10.product = fan)\n+- Relation [customer#8,date#9,items#10,order_id#11L] json\n"
     ]
    }
   ],
   "source": [
    "orders_df.filter(orders_df[\"items.product\"] == \"fan\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code doesn't work! Use [```array_contains```](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.functions.array_contains.html) instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 251.64599609375,
      "end_time": 1573666726393.938
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32778"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import array_contains\n",
    "\n",
    "orders_df.filter(array_contains(\"items.product\", \"fan\")).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Let us try to unnest the data.</b>\n",
    "\n",
    "Unnest the products with [`explode`](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.functions.explode.html?highlight=explode#pyspark.sql.functions.explode).\n",
    "\n",
    "`explode` will generate as many rows as there are elements in the array and match them to other attributes via projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 1255.80712890625,
      "end_time": 1573666787807.612
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+--------+\n",
      "|                   i|        product|order_id|\n",
      "+--------------------+---------------+--------+\n",
      "|      {1.53, fan, 5}|            fan|       0|\n",
      "|{1.33, computer s...|computer screen|       0|\n",
      "|   {1.06, kettle, 6}|         kettle|       0|\n",
      "|{1.96, stuffed an...| stuffed animal|       0|\n",
      "| {1.09, the book, 7}|       the book|       0|\n",
      "+--------------------+---------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "orders_df.select(explode(\"items\").alias(\"i\"), \"i.product\", \"order_id\").orderBy(\"order_id\").limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use this table to filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 746.837158203125,
      "end_time": 1573667003917.751
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39922"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exploded_df = orders_df.select(explode(\"items\").alias(\"i\"), \"i.product\", \"order_id\")\n",
    "exploded_df.filter(exploded_df[\"product\"] == \"fan\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have tried to access the `i.product` column directly using a ```.filter``` right after the ```.select```. That, however, does not work, because the column is not available to ```orders_df``` when creating a clause like ```(orders_df[\"i.product\"] == \"fan\")```. A possible workaround when using Dataframe operations is to use a string clause in ```.filter```, so that the product column will be resolved after it has been added with the ```.select```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 247.906005859375,
      "end_time": 1573667777707.59
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39922"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders_df.select(explode(\"items\").alias(\"i\"), \"i.product\", \"order_id\").filter(\"product = 'fan'\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any ideas why there are more \"fan\" in the `explode` query than the `array_contain` one? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because that there could be more than one \"fan\" types in each order, and we would therefore count some orders with the same `order_id` more than one time. You will find about that when inspecting the `orders.jsonl` data. \n",
    "E.g., \n",
    "```json\n",
    "{\"order_id\": 2, \"date\": \"2016-6-6\", \"customer\": {\"first_name\": \"Brendon\", \"last_name\": \"Sicilia\"}, \"items\": [..., {\"product\": \"fan\", \"quantity\": 7, \"price\": 1.1}, ..., {\"product\": \"fan\", \"quantity\": 8, \"price\": 1.15}]}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Project the nested columns.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 269.365966796875,
      "end_time": 1573669285846.051
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---------+--------+-----+---------------+--------+\n",
      "|order_id|first_name|last_name|    date|price|        product|quantity|\n",
      "+--------+----------+---------+--------+-----+---------------+--------+\n",
      "|       0|   Preston|   Landry|2018-2-4| 1.53|            fan|       5|\n",
      "|       0|   Preston|   Landry|2018-2-4| 1.33|computer screen|       6|\n",
      "|       0|   Preston|   Landry|2018-2-4| 1.06|         kettle|       6|\n",
      "+--------+----------+---------+--------+-----+---------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.select(explode(\"items\").alias(\"i\"), \"*\").select(\n",
    "    \"order_id\", \"customer.*\", \"date\", \"i.*\").limit(3).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Find the average quantity at which each product is purchased. Only show the top 10 products by average quantity. <br> \n",
    "(Hint: You may need to import the function [`desc`](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.functions.desc.html?highlight=desc) from ```pyspark.sql.functions``` to define descending order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 4287.89892578125,
      "end_time": 1573675535490.617
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------------+\n",
      "|        product|    avg(quantity)|\n",
      "+---------------+-----------------+\n",
      "|        toaster|5.515549016184942|\n",
      "|       the book|5.514178678641427|\n",
      "|         kettle|5.512053325314489|\n",
      "|computer screen|5.504839685420448|\n",
      "|     mouse trap|5.503895651308093|\n",
      "|            fan|5.496342868593758|\n",
      "|     headphones|5.485920795060985|\n",
      "|       notebook|5.483182341458532|\n",
      "| whiskey bottle|5.475555222463714|\n",
      "| stuffed animal|5.470854598218753|\n",
      "+---------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "orders_df.select(explode(\"items\").alias(\"i\"), \"*\").select(\n",
    "    \"i.product\", \"i.quantity\"\n",
    ").groupBy(\"product\").avg(\"quantity\").orderBy(desc(\"avg(quantity)\")).limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Find the most expensive order. <br>\n",
    "(Hint: You first build a dataframe by `explode`'ing the items. Then you calculate the total price and aggregate per order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 2290.953125,
      "end_time": 1573669705281.358
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|order_id|        sum(total)|\n",
      "+--------+------------------+\n",
      "|   99636|104.95999999999998|\n",
      "+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exploded_df = orders_df.select(explode(\"items\").alias(\"i\"), \"*\")\n",
    "exploded_df.select(\n",
    "    \"order_id\", (exploded_df[\"i.quantity\"] * exploded_df[\"i.price\"]).alias(\"total\")\n",
    ").groupBy(\"order_id\").sum(\"total\").orderBy(desc(\"sum(total)\")).limit(1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>2. Spark SQL</center>\n",
    "\n",
    "Spark SQL allows the users to formulate their queries using SQL. The requirement is the use of Dataframes, which as said before are similar to relational tables. In addition to a familiar interface, writing queries in SQL might provide better performance than RDDs, inheriting efficiency from the Dataframe operations, while also performing automatic optimization of queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to install the `sparksql` magic command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install sparksql-magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext sparksql_magic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use SQL we need to create a temporary table.\n",
    "\n",
    "<b>Note this table only exists for the current session.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 42.614990234375,
      "end_time": 1573668230627.757
     }
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "orders_df.createOrReplaceTempView(\"orders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, run SQL queries on the registered table `orders`. We will run the same queries as during the previous section, but with SQL.\n",
    "\n",
    "As you can see we can navigate to the nested items with the dot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 11478.6259765625,
      "end_time": 1573665795839.541
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">count(1)</td></tr><tr><td>1960</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "-- Finally, run SQL queries on the registered table \"orders\"\n",
    "-- As you can see we can navigate to the nested items with the dot\n",
    "SELECT count(*)\n",
    "FROM orders\n",
    "WHERE orders.customer.last_name == \"Landry\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about nested arrays?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 2276.55419921875,
      "end_time": 1573666251672.414
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">order_id</td><td style=\"font-weight: bold\">items</td></tr><tr><td>0</td><td>[Row(price=1.53, product=&#x27;fan&#x27;, quantity=5), Row(price=1.33, product=&#x27;computer screen&#x27;, quantity=6), Row(price=1.06, product=&#x27;kettle&#x27;, quantity=6), Row(price=1.96, product=&#x27;stuffed animal&#x27;, quantity=3), Row(price=1.09, product=&#x27;the book&#x27;, quantity=7), Row(price=1.42, product=&#x27;headphones&#x27;, quantity=9), Row(price=1.67, product=&#x27;whiskey bottle&#x27;, quantity=3)]</td></tr><tr><td>1</td><td>[Row(price=1.61, product=&#x27;fan&#x27;, quantity=7), Row(price=1.39, product=&#x27;whiskey bottle&#x27;, quantity=2)]</td></tr><tr><td>2</td><td>[Row(price=1.41, product=&#x27;the book&#x27;, quantity=6), Row(price=1.3, product=&#x27;notebook&#x27;, quantity=5), Row(price=1.1, product=&#x27;fan&#x27;, quantity=7), Row(price=1.5, product=&#x27;stuffed animal&#x27;, quantity=10), Row(price=1.39, product=&#x27;headphones&#x27;, quantity=8), Row(price=1.78, product=&#x27;whiskey bottle&#x27;, quantity=3), Row(price=1.15, product=&#x27;fan&#x27;, quantity=8)]</td></tr><tr><td>3</td><td>[Row(price=1.05, product=&#x27;computer screen&#x27;, quantity=10), Row(price=1.5, product=&#x27;stuffed animal&#x27;, quantity=10), Row(price=1.42, product=&#x27;whiskey bottle&#x27;, quantity=10)]</td></tr><tr><td>4</td><td>[Row(price=1.92, product=&#x27;headphones&#x27;, quantity=2), Row(price=1.44, product=&#x27;fan&#x27;, quantity=2), Row(price=1.84, product=&#x27;kettle&#x27;, quantity=4), Row(price=1.44, product=&#x27;stuffed animal&#x27;, quantity=5)]</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "-- How about nested arrays?\n",
    "SELECT order_id, items\n",
    "FROM orders AS o\n",
    "ORDER BY order_id\n",
    "LIMIT 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to find orders of a fan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 248.202880859375,
      "end_time": 1573666528302.263
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[DATATYPE_MISMATCH.BINARY_OP_DIFF_TYPES] Cannot resolve \"(items.product = fan)\" due to data type mismatch: the left and right operands of the binary operator have incompatible types (\"ARRAY<STRING>\" and \"STRING\").; line 3 pos 6;\n'Aggregate [unresolvedalias(count(1), None)]\n+- 'Filter (items#10.product = fan)\n   +- SubqueryAlias orders\n      +- View (`orders`, [customer#8,date#9,items#10,order_id#11L])\n         +- Relation [customer#8,date#9,items#10,order_id#11L] json\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msparksql\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSELECT count(*)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mFROM orders\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mWHERE items.product = \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2475\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2473\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2474\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2475\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2477\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2478\u001b[0m \u001b[38;5;66;03m# when using magics with decodator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2479\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sparksql_magic/sparksql.py:40\u001b[0m, in \u001b[0;36mSparkSql.sparksql\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactive spark session is not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbind_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_ns\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mor\u001b[39;00m args\u001b[38;5;241m.\u001b[39meager:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcache dataframe with \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m load\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meager\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39meager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlazy\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:1440\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1438\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1439\u001b[0m     litArgs \u001b[38;5;241m=\u001b[39m {k: _to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m {})\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m-> 1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1442\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [DATATYPE_MISMATCH.BINARY_OP_DIFF_TYPES] Cannot resolve \"(items.product = fan)\" due to data type mismatch: the left and right operands of the binary operator have incompatible types (\"ARRAY<STRING>\" and \"STRING\").; line 3 pos 6;\n'Aggregate [unresolvedalias(count(1), None)]\n+- 'Filter (items#10.product = fan)\n   +- SubqueryAlias orders\n      +- View (`orders`, [customer#8,date#9,items#10,order_id#11L])\n         +- Relation [customer#8,date#9,items#10,order_id#11L] json\n"
     ]
    }
   ],
   "source": [
    "%%sparksql \n",
    "SELECT count(*)\n",
    "FROM orders\n",
    "WHERE items.product = \"fan\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code doesn't work! Use [```array_contains```](https://spark.apache.org/docs/latest/api/sql/index.html#array_contains) instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 752.942138671875,
      "end_time": 1573666530734.473
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">count(1)</td></tr><tr><td>32778</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "\n",
    "SELECT count(*)\n",
    "FROM orders\n",
    "WHERE array_contains(items.product, \"fan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to unnest the data.\n",
    "\n",
    "Unnest the products with [`explode`](https://spark.apache.org/docs/latest/api/sql/index.html#explode).\n",
    "\n",
    "`explode` will generate as many rows as there are elements in the array and match them to other attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 772.9169921875,
      "end_time": 1573667016192.464
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">i</td><td style=\"font-weight: bold\">product</td><td style=\"font-weight: bold\">order_id</td></tr><tr><td>Row(price=1.53, product=&#x27;fan&#x27;, quantity=5)</td><td>fan</td><td>0</td></tr><tr><td>Row(price=1.33, product=&#x27;computer screen&#x27;, quantity=6)</td><td>computer screen</td><td>0</td></tr><tr><td>Row(price=1.06, product=&#x27;kettle&#x27;, quantity=6)</td><td>kettle</td><td>0</td></tr><tr><td>Row(price=1.96, product=&#x27;stuffed animal&#x27;, quantity=3)</td><td>stuffed animal</td><td>0</td></tr><tr><td>Row(price=1.09, product=&#x27;the book&#x27;, quantity=7)</td><td>the book</td><td>0</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "SELECT explode(items) as i, i.product, order_id\n",
    "FROM orders\n",
    "ORDER BY order_id\n",
    "limit 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use this table to filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 3281.930908203125,
      "end_time": 1573667022422.047
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">count(1)</td></tr><tr><td>39922</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "-- Filter on product\n",
    "SELECT count(*)\n",
    "    FROM (\n",
    "    SELECT explode(items) as i, i.product, order_id\n",
    "    FROM orders\n",
    "    ORDER BY order_id\n",
    "    )\n",
    "WHERE product = \"fan\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have tried to access the `i.product` column directly in the same ```SELECT``` clause. That, however, does not work, because the column is not available to the ```WHERE``` clause. In order to access the built columns directly, we need to unnest the data and make it part of our ```FROM``` clause. [```LATERAL VIEW```](https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-lateral-view.html) lets us do just that, matching each non-array attribute to an unnested row from the array.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 770.024169921875,
      "end_time": 1573667932258.994
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">customer</td><td style=\"font-weight: bold\">date</td><td style=\"font-weight: bold\">items</td><td style=\"font-weight: bold\">order_id</td><td style=\"font-weight: bold\">flat_items</td></tr><tr><td>Row(first_name=&#x27;Preston&#x27;, last_name=&#x27;Landry&#x27;)</td><td>2018-2-4</td><td>[Row(price=1.53, product=&#x27;fan&#x27;, quantity=5), Row(price=1.33, product=&#x27;computer screen&#x27;, quantity=6), Row(price=1.06, product=&#x27;kettle&#x27;, quantity=6), Row(price=1.96, product=&#x27;stuffed animal&#x27;, quantity=3), Row(price=1.09, product=&#x27;the book&#x27;, quantity=7), Row(price=1.42, product=&#x27;headphones&#x27;, quantity=9), Row(price=1.67, product=&#x27;whiskey bottle&#x27;, quantity=3)]</td><td>0</td><td>Row(price=1.53, product=&#x27;fan&#x27;, quantity=5)</td></tr><tr><td>Row(first_name=&#x27;Jamari&#x27;, last_name=&#x27;Dominguez&#x27;)</td><td>2016-1-8</td><td>[Row(price=1.61, product=&#x27;fan&#x27;, quantity=7), Row(price=1.39, product=&#x27;whiskey bottle&#x27;, quantity=2)]</td><td>1</td><td>Row(price=1.61, product=&#x27;fan&#x27;, quantity=7)</td></tr><tr><td>Row(first_name=&#x27;Brendon&#x27;, last_name=&#x27;Sicilia&#x27;)</td><td>2016-6-6</td><td>[Row(price=1.41, product=&#x27;the book&#x27;, quantity=6), Row(price=1.3, product=&#x27;notebook&#x27;, quantity=5), Row(price=1.1, product=&#x27;fan&#x27;, quantity=7), Row(price=1.5, product=&#x27;stuffed animal&#x27;, quantity=10), Row(price=1.39, product=&#x27;headphones&#x27;, quantity=8), Row(price=1.78, product=&#x27;whiskey bottle&#x27;, quantity=3), Row(price=1.15, product=&#x27;fan&#x27;, quantity=8)]</td><td>2</td><td>Row(price=1.1, product=&#x27;fan&#x27;, quantity=7)</td></tr><tr><td>Row(first_name=&#x27;Brendon&#x27;, last_name=&#x27;Sicilia&#x27;)</td><td>2016-6-6</td><td>[Row(price=1.41, product=&#x27;the book&#x27;, quantity=6), Row(price=1.3, product=&#x27;notebook&#x27;, quantity=5), Row(price=1.1, product=&#x27;fan&#x27;, quantity=7), Row(price=1.5, product=&#x27;stuffed animal&#x27;, quantity=10), Row(price=1.39, product=&#x27;headphones&#x27;, quantity=8), Row(price=1.78, product=&#x27;whiskey bottle&#x27;, quantity=3), Row(price=1.15, product=&#x27;fan&#x27;, quantity=8)]</td><td>2</td><td>Row(price=1.15, product=&#x27;fan&#x27;, quantity=8)</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "SELECT *\n",
    "FROM orders LATERAL VIEW explode(items) as flat_items\n",
    "WHERE flat_items.product = \"fan\"\n",
    "ORDER BY order_id\n",
    "LIMIT 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project the nested columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 2275.98095703125,
      "end_time": 1573667943996.512
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">order_id</td><td style=\"font-weight: bold\">first_name</td><td style=\"font-weight: bold\">last_name</td><td style=\"font-weight: bold\">date</td><td style=\"font-weight: bold\">price</td><td style=\"font-weight: bold\">product</td><td style=\"font-weight: bold\">quantity</td></tr><tr><td>0</td><td>Preston</td><td>Landry</td><td>2018-2-4</td><td>1.53</td><td>fan</td><td>5</td></tr><tr><td>1</td><td>Jamari</td><td>Dominguez</td><td>2016-1-8</td><td>1.61</td><td>fan</td><td>7</td></tr><tr><td>2</td><td>Brendon</td><td>Sicilia</td><td>2016-6-6</td><td>1.1</td><td>fan</td><td>7</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "SELECT order_id, customer.first_name, customer.last_name, date, flat_items.*\n",
    "FROM orders LATERAL VIEW explode(items) item_table as flat_items\n",
    "WHERE flat_items.product = \"fan\"\n",
    "ORDER BY order_id\n",
    "LIMIT 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having built an unnested table, we can now easily aggregate over the previously nested columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Find the average quantity at which each product is purchased. Only show the top 10 products by quantity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 2274.546142578125,
      "end_time": 1573669714658.905
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">product</td><td style=\"font-weight: bold\">avg_quantity</td></tr><tr><td>toaster</td><td>5.515549016184942</td></tr><tr><td>the book</td><td>5.514178678641427</td></tr><tr><td>kettle</td><td>5.512053325314489</td></tr><tr><td>computer screen</td><td>5.504839685420448</td></tr><tr><td>mouse trap</td><td>5.503895651308093</td></tr><tr><td>fan</td><td>5.496342868593758</td></tr><tr><td>headphones</td><td>5.485920795060985</td></tr><tr><td>notebook</td><td>5.483182341458532</td></tr><tr><td>whiskey bottle</td><td>5.475555222463714</td></tr><tr><td>stuffed animal</td><td>5.470854598218753</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "SELECT flat_items.product, AVG(flat_items.quantity) as avg_quantity\n",
    "FROM orders LATERAL VIEW explode(items) flat_table flat_items\n",
    "GROUP BY flat_items.product\n",
    "ORDER BY avg_quantity DESC\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Find the most expensive order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 1268.054931640625,
      "end_time": 1573669716818.317
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">order_id</td><td style=\"font-weight: bold\">total</td></tr><tr><td>99636</td><td>104.95999999999998</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "SELECT order_id, SUM(flat_items.quantity * flat_items.price) as total\n",
    "FROM orders LATERAL VIEW explode(items) flat_table flat_items\n",
    "GROUP BY order_id\n",
    "ORDER BY total desc\n",
    "LIMIT 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7qPak0E_rLuZ"
   },
   "source": [
    "## <center>3. Create Nestedness (Optional)</center>\n",
    "\n",
    "We've already had a look at the solution of dataframes/SparkSQL towards <b>unnesting</b> arrays by using `explode` method. For the other way round, Spark Dataframes / Spark SQL also provide ways for us to nest our data by creating arrays, especially after clauses like `groupBy`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MCUYQYzawGkZ"
   },
   "source": [
    "In traditional PostgreSQL, we have to use one of the aggregation functions (`max, sum, count,`...) to process the result after the `groupBy` operation. For example, for each customer (assume there are no customers with both the same first name and last name), we want to find out the number of distinct dates when they placed an order. You can fill in the queries in both Spark DataFrames and Spark SQL. The query could look like this using [`countDistinct`](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.functions.countDistinct.html?highlight=countdistinct#pyspark.sql.functions.countDistinct):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 11936.587890625,
      "end_time": 1620477905132.344
     }
    },
    "id": "5FI98iZ70Ipm",
    "outputId": "70643201-9a3c-4f55-f782-58ef447d339e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------+\n",
      "|first_name|  last_name|count(date)|\n",
      "+----------+-----------+-----------+\n",
      "|      Zane|       Dahl|          3|\n",
      "|    Dorian|       Dahl|          2|\n",
      "|      Rory|      Dower|          2|\n",
      "|    Morgan|       Miao|          2|\n",
      "|    Ashlyn|      Hatch|          1|\n",
      "|    Landen|  Galatioto|          2|\n",
      "|     Allan|         Po|          4|\n",
      "|  Clarissa|    Sicilia|          2|\n",
      "|     Annie|      Dower|          2|\n",
      "|     Micah|         Mo|          4|\n",
      "|    Morgan|    Poitras|          2|\n",
      "|    Gordon|     Gruber|          2|\n",
      "|Alexandria|Butterfield|          3|\n",
      "|    Thomas|      Dower|          1|\n",
      "|     Ariel|    Coulson|          3|\n",
      "|   Xiomara|  Christofi|          2|\n",
      "|     Rylie|       Dahl|          1|\n",
      "|    Daniel|    Schuler|          1|\n",
      "|     Chana|    Balster|          2|\n",
      "|    Azaria| Berenguier|          2|\n",
      "+----------+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "orders_df.groupBy(\"customer.first_name\", \"customer.last_name\").agg(countDistinct(\"date\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 14957.180908203125,
      "end_time": 1620477984175.839
     }
    },
    "colab": {
     "referenced_widgets": [
      "fd7fe3bdf4c64325a8320ff980c20670",
      "a9efa0cac1714601a48834f0af4eb3fb"
     ]
    },
    "id": "FunDHOxO1zPs",
    "outputId": "8835267b-ab38-4bc5-ce5f-32ca505001bd",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only showing top 20 row(s)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">first_name</td><td style=\"font-weight: bold\">last_name</td><td style=\"font-weight: bold\">count(DISTINCT date)</td></tr><tr><td>Zane</td><td>Dahl</td><td>3</td></tr><tr><td>Dorian</td><td>Dahl</td><td>2</td></tr><tr><td>Rory</td><td>Dower</td><td>2</td></tr><tr><td>Morgan</td><td>Miao</td><td>2</td></tr><tr><td>Ashlyn</td><td>Hatch</td><td>1</td></tr><tr><td>Landen</td><td>Galatioto</td><td>2</td></tr><tr><td>Allan</td><td>Po</td><td>4</td></tr><tr><td>Clarissa</td><td>Sicilia</td><td>2</td></tr><tr><td>Annie</td><td>Dower</td><td>2</td></tr><tr><td>Micah</td><td>Mo</td><td>4</td></tr><tr><td>Morgan</td><td>Poitras</td><td>2</td></tr><tr><td>Gordon</td><td>Gruber</td><td>2</td></tr><tr><td>Alexandria</td><td>Butterfield</td><td>3</td></tr><tr><td>Thomas</td><td>Dower</td><td>1</td></tr><tr><td>Ariel</td><td>Coulson</td><td>3</td></tr><tr><td>Xiomara</td><td>Christofi</td><td>2</td></tr><tr><td>Rylie</td><td>Dahl</td><td>1</td></tr><tr><td>Daniel</td><td>Schuler</td><td>1</td></tr><tr><td>Chana</td><td>Balster</td><td>2</td></tr><tr><td>Azaria</td><td>Berenguier</td><td>2</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "SELECT customer.first_name, customer.last_name, count(DISTINCT date) \n",
    "FROM orders \n",
    "GROUP BY customer.first_name, customer.last_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dsTLU8ml2ORG"
   },
   "source": [
    "But what if we are interested not only in the count of distinct dates, but the actual\n",
    "dates themselves? Luckily Spark Dataframes / Spark SQL do provide us with methods to preserve the original information of the date list. If now we would like to know for each customer, on which dates they placed an order, we shall use [`collect_set`](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.functions.collect_set.html?highlight=collect_set#pyspark.sql.functions.collect_set) method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 1418.15087890625,
      "end_time": 1620478080466.68
     }
    },
    "id": "h1oGXmWt3gss",
    "outputId": "b6c68a25-93a3-4f82-e10a-845b85479b74",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+\n",
      "|first_name|           last_name|   collect_set(date)|\n",
      "+----------+--------------------+--------------------+\n",
      "|     Aaden|              Ardeni|          [2017-5-6]|\n",
      "|     Aaden|              Badash|[2018-3-3, 2017-3...|\n",
      "|     Aaden|             Balster|[2018-5-10, 2017-...|\n",
      "|     Aaden|          Berenguier|         [2017-3-10]|\n",
      "|     Aaden|           Bridgeman|[2018-1-9, 2018-2-6]|\n",
      "|     Aaden|               Cerda|          [2016-4-9]|\n",
      "|     Aaden|           Christofi|[2016-5-7, 2018-2-9]|\n",
      "|     Aaden|             Coulson|[2018-5-7, 2017-4-8]|\n",
      "|     Aaden|                Dahl|          [2017-5-6]|\n",
      "|     Aaden|              Decaro|          [2016-1-4]|\n",
      "|     Aaden|               Dower|          [2018-5-4]|\n",
      "|     Aaden|               Drago|[2018-5-8, 2018-3...|\n",
      "|     Aaden|                Egan|[2018-6-8, 2018-2-4]|\n",
      "|     Aaden|Fernandez cifuentes |          [2016-3-7]|\n",
      "|     Aaden|              Gruber|          [2018-6-2]|\n",
      "|     Aaden|               Hatch|          [2016-1-3]|\n",
      "|     Aaden|               Horah|[2018-4-5, 2016-6-6]|\n",
      "|     Aaden|              Kerman|          [2018-5-6]|\n",
      "|     Aaden|              Landry|[2017-1-8, 2016-6-8]|\n",
      "|     Aaden|               Lemay|[2018-5-5, 2016-6-8]|\n",
      "+----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_set\n",
    "orders_df.groupBy(\"customer.first_name\", \"customer.last_name\").agg(collect_set(\"date\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 10117.97900390625,
      "end_time": 1620478640139.884
     }
    },
    "colab": {
     "referenced_widgets": [
      "5eca7ac4210c44e6b45b37e7cac4bf34",
      "439cfb14c8d64843871c09a1c0be3d60"
     ]
    },
    "id": "aJW1JpzQ3lEV",
    "outputId": "68d8c40e-ba42-4245-94a5-ab49ae60da27",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only showing top 20 row(s)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">first_name</td><td style=\"font-weight: bold\">last_name</td><td style=\"font-weight: bold\">collect_set(date)</td></tr><tr><td>Aaden</td><td>Ardeni</td><td>[&#x27;2017-5-6&#x27;]</td></tr><tr><td>Aaden</td><td>Badash</td><td>[&#x27;2018-3-3&#x27;, &#x27;2017-3-6&#x27;, &#x27;2016-5-3&#x27;]</td></tr><tr><td>Aaden</td><td>Balster</td><td>[&#x27;2018-5-10&#x27;, &#x27;2017-6-3&#x27;]</td></tr><tr><td>Aaden</td><td>Berenguier</td><td>[&#x27;2017-3-10&#x27;]</td></tr><tr><td>Aaden</td><td>Bridgeman</td><td>[&#x27;2018-1-9&#x27;, &#x27;2018-2-6&#x27;]</td></tr><tr><td>Aaden</td><td>Cerda</td><td>[&#x27;2016-4-9&#x27;]</td></tr><tr><td>Aaden</td><td>Christofi</td><td>[&#x27;2016-5-7&#x27;, &#x27;2018-2-9&#x27;]</td></tr><tr><td>Aaden</td><td>Coulson</td><td>[&#x27;2018-5-7&#x27;, &#x27;2017-4-8&#x27;]</td></tr><tr><td>Aaden</td><td>Dahl</td><td>[&#x27;2017-5-6&#x27;]</td></tr><tr><td>Aaden</td><td>Decaro</td><td>[&#x27;2016-1-4&#x27;]</td></tr><tr><td>Aaden</td><td>Dower</td><td>[&#x27;2018-5-4&#x27;]</td></tr><tr><td>Aaden</td><td>Drago</td><td>[&#x27;2018-5-8&#x27;, &#x27;2018-3-3&#x27;, &#x27;2016-2-10&#x27;, &#x27;2016-5-8&#x27;]</td></tr><tr><td>Aaden</td><td>Egan</td><td>[&#x27;2018-6-8&#x27;, &#x27;2018-2-4&#x27;]</td></tr><tr><td>Aaden</td><td>Fernandez cifuentes </td><td>[&#x27;2016-3-7&#x27;]</td></tr><tr><td>Aaden</td><td>Gruber</td><td>[&#x27;2018-6-2&#x27;]</td></tr><tr><td>Aaden</td><td>Hatch</td><td>[&#x27;2016-1-3&#x27;]</td></tr><tr><td>Aaden</td><td>Horah</td><td>[&#x27;2018-4-5&#x27;, &#x27;2016-6-6&#x27;]</td></tr><tr><td>Aaden</td><td>Kerman</td><td>[&#x27;2018-5-6&#x27;]</td></tr><tr><td>Aaden</td><td>Landry</td><td>[&#x27;2017-1-8&#x27;, &#x27;2016-6-8&#x27;]</td></tr><tr><td>Aaden</td><td>Lemay</td><td>[&#x27;2018-5-5&#x27;, &#x27;2016-6-8&#x27;]</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "SELECT customer.first_name, customer.last_name, collect_set(date)\n",
    "FROM orders \n",
    "GROUP BY customer.first_name, customer.last_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t2rTR-O78gy3"
   },
   "source": [
    "For some other cases where we want to preserve all the entries rather than the de-duplicated ones, we can use  [`collect_list`](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.functions.collect_list.html?highlight=collect_list#pyspark.sql.functions.collect_list) method. For example, for each date we want to record the last names of customers. Since two customers might share the same last name, we need to collect all of them. The query should look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 12271.528076171875,
      "end_time": 1620480910312.875
     }
    },
    "id": "X_PB-uAH9I5M",
    "outputId": "f76c10ee-f085-4121-87af-0a0846061efb",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------------------+\n",
      "|     date|collect_list(customer.last_name)|\n",
      "+---------+--------------------------------+\n",
      "| 2016-1-1|            [Suchoff, Lowe, D...|\n",
      "|2016-1-10|            [Schuler, Macmaho...|\n",
      "| 2016-1-2|            [Horah, Gottardo,...|\n",
      "| 2016-1-3|            [Marinko, Mignani...|\n",
      "| 2016-1-4|            [Curlin, Gruber, ...|\n",
      "| 2016-1-5|            [Suchoff, Srivast...|\n",
      "| 2016-1-6|            [Lowe, Buddha, Mc...|\n",
      "| 2016-1-7|            [Christofi, Poitr...|\n",
      "| 2016-1-8|            [Ardeni, Gruber, ...|\n",
      "| 2016-1-9|            [Cerda, Hatch, Co...|\n",
      "| 2016-2-1|            [Lemay, Rosenbloo...|\n",
      "|2016-2-10|            [Dower, Dahl, Cor...|\n",
      "| 2016-2-2|            [Miao, Coulson, W...|\n",
      "| 2016-2-3|            [Macmahon, Sicili...|\n",
      "| 2016-2-4|            [Suchoff, Curlin,...|\n",
      "| 2016-2-5|            [Dahl, Findley, M...|\n",
      "| 2016-2-6|            [Dahl, Landry, Co...|\n",
      "| 2016-2-7|            [Lowe, Poitras, L...|\n",
      "| 2016-2-8|            [Poitras, Coulson...|\n",
      "| 2016-2-9|            [Egan, Rosenbloom...|\n",
      "+---------+--------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_list\n",
    "orders_df.groupBy(\"date\").agg(collect_list(\"customer.last_name\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 30449.055908203125,
      "end_time": 1620481513099.958
     }
    },
    "colab": {
     "referenced_widgets": [
      "a2f4e8e80a2e424ba0e552e89b16a8a7",
      "2b9d668dbcfa4a639a140d117b5e3902"
     ]
    },
    "id": "z7DSOsKC9dmF",
    "outputId": "eae40ebd-63f9-4ec1-9742-f1bb391d1071",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">date</td><td style=\"font-weight: bold\">collect_list(customer.last_name)</td></tr><tr><td>2016-1-1</td><td>[&#x27;Suchoff&#x27;, &#x27;Lowe&#x27;, &#x27;Dominguez&#x27;, &#x27;Hatch&#x27;, &#x27;Poitras&#x27;, &#x27;Poitras&#x27;, &#x27;Christofi&#x27;, &#x27;Dahlstedt&#x27;, &#x27;Berenguier&#x27;, &#x27;Fernandez cifuentes &#x27;, &#x27;Hirsch&#x27;, &#x27;Suchoff&#x27;, &#x27;Lemay&#x27;, &#x27;Lowe&#x27;, &#x27;Lemay&#x27;, &#x27;Gruber&#x27;, &#x27;Dominguez&#x27;, &#x27;Badash&#x27;, &#x27;Mignani&#x27;, &#x27;Gottardo&#x27;, &#x27;Ardeni&#x27;, &#x27;Schuler&#x27;, &#x27;Gottardo&#x27;, &#x27;Christofi&#x27;, &#x27;Schuler&#x27;, &#x27;Kerman&#x27;, &#x27;Badash&#x27;, &#x27;Buddha&#x27;, &#x27;Delossa&#x27;, &#x27;Cerda&#x27;, &#x27;Gruber&#x27;, &#x27;Gottardo&#x27;, &#x27;Coulson&#x27;, &#x27;Cerda&#x27;, &#x27;Bridgeman&#x27;, &#x27;Mo&#x27;, &#x27;Mayer&#x27;, &#x27;Sicilia&#x27;, &#x27;Dominguez&#x27;, &#x27;Poitras&#x27;, &#x27;Decaro&#x27;, &#x27;Curlin&#x27;, &#x27;Galatioto&#x27;, &#x27;Curlin&#x27;, &#x27;Whitla&#x27;, &#x27;Gottardo&#x27;, &#x27;Landry&#x27;, &#x27;Sicilia&#x27;, &#x27;Curlin&#x27;, &#x27;Fernandez cifuentes &#x27;, &#x27;Christofi&#x27;, &#x27;Neuman&#x27;, &#x27;Dominguez&#x27;, &#x27;Egan&#x27;, &#x27;Marinko&#x27;, &#x27;Ho-stuart&#x27;, &#x27;Horah&#x27;, &#x27;Landry&#x27;, &#x27;Ho-stuart&#x27;, &#x27;Cerda&#x27;, &#x27;Buddha&#x27;, &#x27;Hirsch&#x27;, &#x27;Landry&#x27;, &#x27;Neuman&#x27;, &#x27;Dower&#x27;, &#x27;Dower&#x27;, &#x27;Coulson&#x27;, &#x27;Drago&#x27;, &#x27;Macmahon&#x27;, &#x27;Zapata&#x27;, &#x27;Horah&#x27;, &#x27;Decaro&#x27;, &#x27;Hatch&#x27;, &#x27;Zapata&#x27;, &#x27;Drago&#x27;, &#x27;Findley&#x27;, &#x27;Hatch&#x27;, &#x27;Poitras&#x27;, &#x27;Bridgeman&#x27;, &#x27;Ardeni&#x27;, &#x27;Schuler&#x27;, &#x27;Sicilia&#x27;, &#x27;Srivastava&#x27;, &#x27;Suchoff&#x27;, &#x27;Macmahon&#x27;, &#x27;Dower&#x27;, &#x27;Dahl&#x27;, &#x27;Egan&#x27;, &#x27;Zapata&#x27;, &#x27;Macmahon&#x27;, &#x27;Mignani&#x27;, &#x27;Po&#x27;, &#x27;Buddha&#x27;, &#x27;Balster&#x27;, &#x27;Egan&#x27;, &#x27;Mignani&#x27;, &#x27;Balster&#x27;, &#x27;Gruber&#x27;, &#x27;Galatioto&#x27;, &#x27;Zapata&#x27;, &#x27;Gottardo&#x27;, &#x27;Mignani&#x27;, &#x27;Drago&#x27;, &#x27;Bridgeman&#x27;, &#x27;Zapata&#x27;, &#x27;Corfas&#x27;, &#x27;Poitras&#x27;, &#x27;Sicilia&#x27;, &#x27;Dahl&#x27;, &#x27;Macmahon&#x27;, &#x27;Berenguier&#x27;, &#x27;Hirsch&#x27;, &#x27;Landry&#x27;, &#x27;Mc namara&#x27;, &#x27;Berenguier&#x27;, &#x27;Miao&#x27;, &#x27;Zapata&#x27;, &#x27;Dower&#x27;, &#x27;Dower&#x27;, &#x27;Hatch&#x27;, &#x27;Butterfield&#x27;, &#x27;Miao&#x27;, &#x27;Christofi&#x27;, &#x27;Dahl&#x27;, &#x27;Hirsch&#x27;, &#x27;Whitla&#x27;, &#x27;Dahl&#x27;, &#x27;Fernandez cifuentes &#x27;, &#x27;Mignani&#x27;, &#x27;Butterfield&#x27;, &#x27;Macmahon&#x27;, &#x27;Delossa&#x27;, &#x27;Po&#x27;, &#x27;Schuler&#x27;, &#x27;Findley&#x27;, &#x27;Hirsch&#x27;, &#x27;Coulson&#x27;, &#x27;Gruber&#x27;, &#x27;Dower&#x27;, &#x27;Coulson&#x27;, &#x27;Macmahon&#x27;, &#x27;Macmahon&#x27;, &#x27;Schuler&#x27;, &#x27;Berenguier&#x27;, &#x27;Dower&#x27;, &#x27;Kerman&#x27;, &#x27;Mo&#x27;, &#x27;Poitras&#x27;, &#x27;Zapata&#x27;, &#x27;Landry&#x27;, &#x27;Horah&#x27;, &#x27;Butterfield&#x27;, &#x27;Zapata&#x27;, &#x27;Marinko&#x27;, &#x27;Dominguez&#x27;, &#x27;Dominguez&#x27;, &#x27;Mayer&#x27;, &#x27;Hatch&#x27;, &#x27;Bridgeman&#x27;, &#x27;Landry&#x27;, &#x27;Decaro&#x27;, &#x27;Fernandez cifuentes &#x27;, &#x27;Fernandez cifuentes &#x27;, &#x27;Macmahon&#x27;, &#x27;Berenguier&#x27;, &#x27;Horah&#x27;, &#x27;Hatch&#x27;, &#x27;Hirsch&#x27;, &#x27;Marinko&#x27;, &#x27;Dahl&#x27;, &#x27;Srivastava&#x27;, &#x27;Mayer&#x27;, &#x27;Landry&#x27;, &#x27;Fernandez cifuentes &#x27;, &#x27;Landry&#x27;, &#x27;Egan&#x27;, &#x27;Schuler&#x27;, &#x27;Srivastava&#x27;, &#x27;Cerda&#x27;, &#x27;Lowe&#x27;, &#x27;Butterfield&#x27;, &#x27;Macmahon&#x27;, &#x27;Kerman&#x27;, &#x27;Gottardo&#x27;, &#x27;Hatch&#x27;, &#x27;Drago&#x27;, &#x27;Fernandez cifuentes &#x27;, &#x27;Ho-stuart&#x27;, &#x27;Sicilia&#x27;, &#x27;Buddha&#x27;, &#x27;Cerda&#x27;, &#x27;Berenguier&#x27;, &#x27;Ardeni&#x27;, &#x27;Hirsch&#x27;, &#x27;Macmahon&#x27;, &#x27;Delossa&#x27;, &#x27;Delossa&#x27;, &#x27;Lemay&#x27;, &#x27;Whitla&#x27;, &#x27;Corfas&#x27;, &#x27;Dominguez&#x27;, &#x27;Mc namara&#x27;, &#x27;Curlin&#x27;, &#x27;Marinko&#x27;, &#x27;Schuler&#x27;, &#x27;Butterfield&#x27;, &#x27;Badash&#x27;, &#x27;Coulson&#x27;, &#x27;Po&#x27;, &#x27;Whitla&#x27;, &#x27;Kerman&#x27;, &#x27;Mayer&#x27;, &#x27;Kerman&#x27;, &#x27;Findley&#x27;, &#x27;Mo&#x27;, &#x27;Curlin&#x27;, &#x27;Hirsch&#x27;, &#x27;Egan&#x27;, &#x27;Horah&#x27;, &#x27;Cerda&#x27;, &#x27;Mayer&#x27;, &#x27;Fernandez cifuentes &#x27;, &#x27;Fernandez cifuentes &#x27;, &#x27;Lowe&#x27;, &#x27;Dahl&#x27;, &#x27;Dower&#x27;, &#x27;Buddha&#x27;, &#x27;Kerman&#x27;, &#x27;Hatch&#x27;, &#x27;Egan&#x27;, &#x27;Corfas&#x27;, &#x27;Berenguier&#x27;, &#x27;Ardeni&#x27;, &#x27;Butterfield&#x27;, &#x27;Corfas&#x27;, &#x27;Mc namara&#x27;, &#x27;Fernandez cifuentes &#x27;, &#x27;Horah&#x27;, &#x27;Dower&#x27;, &#x27;Landry&#x27;, &#x27;Suchoff&#x27;, &#x27;Badash&#x27;, &#x27;Fernandez cifuentes &#x27;, &#x27;Poitras&#x27;, &#x27;Lowe&#x27;, &#x27;Landry&#x27;, &#x27;Delossa&#x27;, &#x27;Butterfield&#x27;, &#x27;Sicilia&#x27;, &#x27;Zapata&#x27;, &#x27;Suchoff&#x27;, &#x27;Zapata&#x27;, &#x27;Hirsch&#x27;, &#x27;Zapata&#x27;, &#x27;Curlin&#x27;, &#x27;Christofi&#x27;, &#x27;Poitras&#x27;, &#x27;Hatch&#x27;, &#x27;Horah&#x27;, &#x27;Mc namara&#x27;, &#x27;Hirsch&#x27;, &#x27;Curlin&#x27;, &#x27;Miao&#x27;, &#x27;Kerman&#x27;, &#x27;Delossa&#x27;, &#x27;Kerman&#x27;, &#x27;Drago&#x27;, &#x27;Bridgeman&#x27;, &#x27;Kerman&#x27;, &#x27;Whitla&#x27;, &#x27;Mc namara&#x27;, &#x27;Rosenbloom&#x27;, &#x27;Dominguez&#x27;, &#x27;Mo&#x27;, &#x27;Christofi&#x27;, &#x27;Po&#x27;, &#x27;Bridgeman&#x27;, &#x27;Badash&#x27;, &#x27;Findley&#x27;, &#x27;Findley&#x27;, &#x27;Horah&#x27;, &#x27;Lowe&#x27;, &#x27;Drago&#x27;, &#x27;Berenguier&#x27;, &#x27;Lowe&#x27;, &#x27;Findley&#x27;, &#x27;Macmahon&#x27;, &#x27;Cerda&#x27;, &#x27;Hirsch&#x27;, &#x27;Galatioto&#x27;, &#x27;Galatioto&#x27;, &#x27;Poitras&#x27;, &#x27;Coulson&#x27;, &#x27;Gruber&#x27;, &#x27;Miao&#x27;, &#x27;Sicilia&#x27;, &#x27;Rosenbloom&#x27;, &#x27;Christofi&#x27;, &#x27;Whitla&#x27;, &#x27;Whitla&#x27;, &#x27;Dominguez&#x27;, &#x27;Zapata&#x27;, &#x27;Po&#x27;, &#x27;Lowe&#x27;, &#x27;Marinko&#x27;, &#x27;Dominguez&#x27;, &#x27;Schuler&#x27;, &#x27;Decaro&#x27;, &#x27;Srivastava&#x27;, &#x27;Corfas&#x27;, &#x27;Landry&#x27;, &#x27;Ho-stuart&#x27;, &#x27;Ho-stuart&#x27;, &#x27;Mignani&#x27;, &#x27;Srivastava&#x27;, &#x27;Lemay&#x27;, &#x27;Dahlstedt&#x27;, &#x27;Mignani&#x27;, &#x27;Mayer&#x27;, &#x27;Galatioto&#x27;, &#x27;Gruber&#x27;, &#x27;Ho-stuart&#x27;, &#x27;Bridgeman&#x27;, &#x27;Buddha&#x27;, &#x27;Macmahon&#x27;, &#x27;Po&#x27;, &#x27;Rosenbloom&#x27;, &#x27;Mo&#x27;, &#x27;Horah&#x27;, &#x27;Dahlstedt&#x27;, &#x27;Suchoff&#x27;, &#x27;Gruber&#x27;, &#x27;Suchoff&#x27;, &#x27;Dower&#x27;, &#x27;Fernandez cifuentes &#x27;, &#x27;Lowe&#x27;, &#x27;Mo&#x27;, &#x27;Balster&#x27;, &#x27;Berenguier&#x27;, &#x27;Mignani&#x27;, &#x27;Mignani&#x27;, &#x27;Dahlstedt&#x27;, &#x27;Dower&#x27;, &#x27;Drago&#x27;, &#x27;Christofi&#x27;, &#x27;Marinko&#x27;, &#x27;Miao&#x27;, &#x27;Cerda&#x27;, &#x27;Sicilia&#x27;, &#x27;Landry&#x27;, &#x27;Cerda&#x27;, &#x27;Whitla&#x27;, &#x27;Berenguier&#x27;, &#x27;Neuman&#x27;, &#x27;Egan&#x27;, &#x27;Mignani&#x27;, &#x27;Findley&#x27;, &#x27;Neuman&#x27;, &#x27;Corfas&#x27;, &#x27;Dahlstedt&#x27;, &#x27;Poitras&#x27;, &#x27;Decaro&#x27;, &#x27;Butterfield&#x27;, &#x27;Christofi&#x27;, &#x27;Miao&#x27;, &#x27;Mc namara&#x27;, &#x27;Findley&#x27;, &#x27;Neuman&#x27;, &#x27;Zapata&#x27;, &#x27;Coulson&#x27;, &#x27;Landry&#x27;, &#x27;Dower&#x27;, &#x27;Gottardo&#x27;, &#x27;Coulson&#x27;, &#x27;Horah&#x27;, &#x27;Dahl&#x27;, &#x27;Suchoff&#x27;, &#x27;Horah&#x27;, &#x27;Curlin&#x27;, &#x27;Cerda&#x27;, &#x27;Poitras&#x27;, &#x27;Lowe&#x27;, &#x27;Kerman&#x27;, &#x27;Findley&#x27;, &#x27;Delossa&#x27;, &#x27;Egan&#x27;, &#x27;Mignani&#x27;, &#x27;Suchoff&#x27;, &#x27;Mo&#x27;, &#x27;Dower&#x27;, &#x27;Ho-stuart&#x27;, &#x27;Corfas&#x27;, &#x27;Dominguez&#x27;, &#x27;Decaro&#x27;, &#x27;Schuler&#x27;, &#x27;Dominguez&#x27;, &#x27;Schuler&#x27;, &#x27;Mo&#x27;, &#x27;Buddha&#x27;, &#x27;Gottardo&#x27;, &#x27;Bridgeman&#x27;, &#x27;Horah&#x27;, &#x27;Mc namara&#x27;, &#x27;Whitla&#x27;, &#x27;Rosenbloom&#x27;, &#x27;Butterfield&#x27;, &#x27;Hirsch&#x27;, &#x27;Dower&#x27;, &#x27;Marinko&#x27;, &#x27;Berenguier&#x27;, &#x27;Christofi&#x27;, &#x27;Rosenbloom&#x27;, &#x27;Coulson&#x27;, &#x27;Lowe&#x27;, &#x27;Gottardo&#x27;, &#x27;Mo&#x27;, &#x27;Kerman&#x27;, &#x27;Landry&#x27;, &#x27;Drago&#x27;, &#x27;Dominguez&#x27;, &#x27;Badash&#x27;, &#x27;Sicilia&#x27;, &#x27;Corfas&#x27;, &#x27;Decaro&#x27;, &#x27;Drago&#x27;, &#x27;Lowe&#x27;, &#x27;Dominguez&#x27;, &#x27;Badash&#x27;, &#x27;Neuman&#x27;, &#x27;Egan&#x27;, &#x27;Lowe&#x27;, &#x27;Bridgeman&#x27;, &#x27;Coulson&#x27;, &#x27;Miao&#x27;, &#x27;Mayer&#x27;, &#x27;Horah&#x27;, &#x27;Drago&#x27;, &#x27;Horah&#x27;, &#x27;Marinko&#x27;, &#x27;Landry&#x27;, &#x27;Schuler&#x27;, &#x27;Berenguier&#x27;, &#x27;Drago&#x27;, &#x27;Dahlstedt&#x27;, &#x27;Dominguez&#x27;, &#x27;Hatch&#x27;, &#x27;Sicilia&#x27;, &#x27;Bridgeman&#x27;, &#x27;Mo&#x27;, &#x27;Decaro&#x27;, &#x27;Findley&#x27;, &#x27;Dominguez&#x27;, &#x27;Dahlstedt&#x27;, &#x27;Mc namara&#x27;, &#x27;Berenguier&#x27;, &#x27;Mo&#x27;, &#x27;Fernandez cifuentes &#x27;, &#x27;Rosenbloom&#x27;, &#x27;Cerda&#x27;, &#x27;Mayer&#x27;, &#x27;Miao&#x27;, &#x27;Decaro&#x27;, &#x27;Mignani&#x27;, &#x27;Coulson&#x27;, &#x27;Po&#x27;, &#x27;Mc namara&#x27;, &#x27;Gottardo&#x27;, &#x27;Dahlstedt&#x27;, &#x27;Corfas&#x27;, &#x27;Galatioto&#x27;, &#x27;Lowe&#x27;, &#x27;Buddha&#x27;, &#x27;Marinko&#x27;, &#x27;Drago&#x27;, &#x27;Macmahon&#x27;, &#x27;Gruber&#x27;, &#x27;Curlin&#x27;, &#x27;Decaro&#x27;, &#x27;Landry&#x27;, &#x27;Ho-stuart&#x27;, &#x27;Rosenbloom&#x27;, &#x27;Berenguier&#x27;, &#x27;Butterfield&#x27;, &#x27;Dahlstedt&#x27;, &#x27;Kerman&#x27;, &#x27;Gruber&#x27;, &#x27;Dahlstedt&#x27;, &#x27;Coulson&#x27;, &#x27;Suchoff&#x27;, &#x27;Mayer&#x27;, &#x27;Hirsch&#x27;, &#x27;Curlin&#x27;, &#x27;Lowe&#x27;, &#x27;Butterfield&#x27;, &#x27;Findley&#x27;, &#x27;Buddha&#x27;, &#x27;Mo&#x27;, &#x27;Mc namara&#x27;, &#x27;Whitla&#x27;, &#x27;Findley&#x27;, &#x27;Lowe&#x27;, &#x27;Ardeni&#x27;, &#x27;Hatch&#x27;, &#x27;Dahlstedt&#x27;, &#x27;Lemay&#x27;, &#x27;Ho-stuart&#x27;, &#x27;Whitla&#x27;, &#x27;Galatioto&#x27;, &#x27;Mignani&#x27;, &#x27;Poitras&#x27;, &#x27;Horah&#x27;, &#x27;Rosenbloom&#x27;, &#x27;Berenguier&#x27;, &#x27;Srivastava&#x27;, &#x27;Buddha&#x27;, &#x27;Landry&#x27;, &#x27;Drago&#x27;, &#x27;Mo&#x27;, &#x27;Cerda&#x27;, &#x27;Hatch&#x27;, &#x27;Buddha&#x27;, &#x27;Poitras&#x27;, &#x27;Srivastava&#x27;, &#x27;Rosenbloom&#x27;, &#x27;Decaro&#x27;, &#x27;Schuler&#x27;, &#x27;Whitla&#x27;, &#x27;Mayer&#x27;, &#x27;Marinko&#x27;, &#x27;Delossa&#x27;, &#x27;Galatioto&#x27;, &#x27;Srivastava&#x27;, &#x27;Bridgeman&#x27;, &#x27;Egan&#x27;, &#x27;Balster&#x27;, &#x27;Gottardo&#x27;, &#x27;Horah&#x27;, &#x27;Mc namara&#x27;, &#x27;Mignani&#x27;, &#x27;Schuler&#x27;, &#x27;Ho-stuart&#x27;, &#x27;Butterfield&#x27;, &#x27;Macmahon&#x27;, &#x27;Gruber&#x27;, &#x27;Lowe&#x27;, &#x27;Hirsch&#x27;, &#x27;Rosenbloom&#x27;, &#x27;Gottardo&#x27;, &#x27;Fernandez cifuentes &#x27;, &#x27;Findley&#x27;, &#x27;Mignani&#x27;, &#x27;Corfas&#x27;, &#x27;Balster&#x27;, &#x27;Lemay&#x27;, &#x27;Lemay&#x27;, &#x27;Horah&#x27;, &#x27;Mo&#x27;, &#x27;Suchoff&#x27;, &#x27;Coulson&#x27;, &#x27;Galatioto&#x27;, &#x27;Berenguier&#x27;, &#x27;Hirsch&#x27;]</td></tr><tr><td>2016-1-10</td><td>[&#x27;Schuler&#x27;, &#x27;Macmahon&#x27;, &#x27;Coulson&#x27;, &#x27;Christofi&#x27;, &#x27;Dower&#x27;, &#x27;Sicilia&#x27;, &#x27;Badash&#x27;, &#x27;Delossa&#x27;, &#x27;Lemay&#x27;, &#x27;Mignani&#x27;, &#x27;Mc namara&#x27;, &#x27;Fernandez cifuentes &#x27;, &#x27;Sicilia&#x27;, &#x27;Bridgeman&#x27;, &#x27;Neuman&#x27;, &#x27;Findley&#x27;, &#x27;Dower&#x27;, &#x27;Ho-stuart&#x27;, &#x27;Drago&#x27;, &#x27;Curlin&#x27;, &#x27;Poitras&#x27;, &#x27;Schuler&#x27;, &#x27;Kerman&#x27;, &#x27;Christofi&#x27;, &#x27;Christofi&#x27;, &#x27;Mignani&#x27;, &#x27;Kerman&#x27;, &#x27;Fernandez cifuentes &#x27;, &#x27;Dominguez&#x27;, &#x27;Schuler&#x27;, &#x27;Galatioto&#x27;, &#x27;Cerda&#x27;, &#x27;Mo&#x27;, &#x27;Christofi&#x27;, &#x27;Galatioto&#x27;, &#x27;Whitla&#x27;, &#x27;Galatioto&#x27;, &#x27;Landry&#x27;, &#x27;Berenguier&#x27;, &#x27;Macmahon&#x27;, &#x27;Poitras&#x27;, &#x27;Drago&#x27;, &#x27;Hirsch&#x27;, &#x27;Suchoff&#x27;, &#x27;Delossa&#x27;, &#x27;Mayer&#x27;, &#x27;Drago&#x27;, &#x27;Miao&#x27;, &#x27;Kerman&#x27;, &#x27;Mc namara&#x27;, &#x27;Lowe&#x27;, &#x27;Dahl&#x27;, &#x27;Buddha&#x27;, &#x27;Badash&#x27;, &#x27;Gottardo&#x27;, &#x27;Mignani&#x27;, &#x27;Drago&#x27;, &#x27;Fernandez cifuentes &#x27;, &#x27;Dahl&#x27;, &#x27;Corfas&#x27;, &#x27;Srivastava&#x27;, &#x27;Dower&#x27;, &#x27;Lowe&#x27;, &#x27;Mignani&#x27;, &#x27;Christofi&#x27;, &#x27;Mo&#x27;, &#x27;Sicilia&#x27;, &#x27;Ho-stuart&#x27;, &#x27;Christofi&#x27;, &#x27;Corfas&#x27;, &#x27;Mc namara&#x27;, &#x27;Ho-stuart&#x27;, &#x27;Zapata&#x27;, &#x27;Ho-stuart&#x27;, &#x27;Ardeni&#x27;, &#x27;Rosenbloom&#x27;, &#x27;Fernandez cifuentes &#x27;, &#x27;Balster&#x27;, &#x27;Christofi&#x27;, &#x27;Buddha&#x27;, &#x27;Mc namara&#x27;, &#x27;Lowe&#x27;, &#x27;Schuler&#x27;, &#x27;Egan&#x27;, &#x27;Gruber&#x27;, &#x27;Schuler&#x27;, &#x27;Macmahon&#x27;, &#x27;Cerda&#x27;, &#x27;Sicilia&#x27;, &#x27;Gottardo&#x27;, &#x27;Ardeni&#x27;, &#x27;Bridgeman&#x27;, &#x27;Zapata&#x27;, &#x27;Butterfield&#x27;, &#x27;Mc namara&#x27;, &#x27;Balster&#x27;, &#x27;Lowe&#x27;, &#x27;Srivastava&#x27;, &#x27;Berenguier&#x27;, &#x27;Drago&#x27;, &#x27;Hatch&#x27;, &#x27;Cerda&#x27;, &#x27;Bridgeman&#x27;, &#x27;Buddha&#x27;, &#x27;Whitla&#x27;, &#x27;Dower&#x27;, &#x27;Schuler&#x27;, &#x27;Zapata&#x27;, &#x27;Lowe&#x27;, &#x27;Srivastava&#x27;, &#x27;Dahl&#x27;, &#x27;Lowe&#x27;, &#x27;Hirsch&#x27;, &#x27;Neuman&#x27;, &#x27;Fernandez cifuentes &#x27;, &#x27;Buddha&#x27;, &#x27;Galatioto&#x27;, &#x27;Balster&#x27;, &#x27;Delossa&#x27;, &#x27;Dower&#x27;, &#x27;Dower&#x27;, &#x27;Ardeni&#x27;, &#x27;Buddha&#x27;, &#x27;Macmahon&#x27;, &#x27;Ho-stuart&#x27;, &#x27;Delossa&#x27;, &#x27;Coulson&#x27;, &#x27;Dahlstedt&#x27;, &#x27;Gottardo&#x27;, &#x27;Sicilia&#x27;, &#x27;Findley&#x27;, &#x27;Coulson&#x27;, &#x27;Landry&#x27;, &#x27;Hirsch&#x27;, &#x27;Egan&#x27;, &#x27;Drago&#x27;, &#x27;Mo&#x27;, &#x27;Gottardo&#x27;, &#x27;Neuman&#x27;, &#x27;Rosenbloom&#x27;, &#x27;Mayer&#x27;, &#x27;Findley&#x27;, &#x27;Fernandez cifuentes &#x27;, &#x27;Cerda&#x27;, &#x27;Hirsch&#x27;, &#x27;Lemay&#x27;, &#x27;Egan&#x27;, &#x27;Marinko&#x27;, &#x27;Cerda&#x27;, &#x27;Berenguier&#x27;, &#x27;Badash&#x27;, &#x27;Dower&#x27;, &#x27;Rosenbloom&#x27;, &#x27;Kerman&#x27;, &#x27;Lowe&#x27;, &#x27;Berenguier&#x27;, &#x27;Dahl&#x27;, &#x27;Fernandez cifuentes &#x27;, &#x27;Dahl&#x27;, &#x27;Butterfield&#x27;, &#x27;Neuman&#x27;, &#x27;Egan&#x27;, &#x27;Balster&#x27;, &#x27;Dominguez&#x27;, &#x27;Whitla&#x27;, &#x27;Findley&#x27;, &#x27;Fernandez cifuentes &#x27;, &#x27;Poitras&#x27;, &#x27;Balster&#x27;, &#x27;Neuman&#x27;, &#x27;Balster&#x27;, &#x27;Christofi&#x27;, &#x27;Lemay&#x27;, &#x27;Po&#x27;, &#x27;Neuman&#x27;, &#x27;Neuman&#x27;, &#x27;Schuler&#x27;, &#x27;Po&#x27;, &#x27;Delossa&#x27;, &#x27;Dominguez&#x27;, &#x27;Buddha&#x27;, &#x27;Galatioto&#x27;, &#x27;Berenguier&#x27;, &#x27;Buddha&#x27;, &#x27;Hatch&#x27;, &#x27;Dower&#x27;, &#x27;Po&#x27;, &#x27;Macmahon&#x27;, &#x27;Zapata&#x27;, &#x27;Bridgeman&#x27;, &#x27;Marinko&#x27;, &#x27;Srivastava&#x27;, &#x27;Coulson&#x27;, &#x27;Delossa&#x27;, &#x27;Neuman&#x27;, &#x27;Fernandez cifuentes &#x27;, &#x27;Marinko&#x27;, &#x27;Po&#x27;, &#x27;Srivastava&#x27;, &#x27;Mayer&#x27;, &#x27;Fernandez cifuentes &#x27;, &#x27;Balster&#x27;, &#x27;Poitras&#x27;, &#x27;Dower&#x27;, &#x27;Rosenbloom&#x27;, &#x27;Drago&#x27;, &#x27;Mo&#x27;, &#x27;Hirsch&#x27;, &#x27;Rosenbloom&#x27;, &#x27;Butterfield&#x27;, &#x27;Galatioto&#x27;, &#x27;Egan&#x27;, &#x27;Fernandez cifuentes &#x27;, &#x27;Rosenbloom&#x27;, &#x27;Mayer&#x27;, &#x27;Rosenbloom&#x27;, &#x27;Miao&#x27;, &#x27;Drago&#x27;, &#x27;Galatioto&#x27;, &#x27;Kerman&#x27;, &#x27;Galatioto&#x27;, &#x27;Corfas&#x27;, &#x27;Hatch&#x27;, &#x27;Dahlstedt&#x27;, &#x27;Horah&#x27;, &#x27;Dahl&#x27;, &#x27;Schuler&#x27;, &#x27;Mayer&#x27;, &#x27;Dominguez&#x27;, &#x27;Miao&#x27;, &#x27;Egan&#x27;, &#x27;Rosenbloom&#x27;, &#x27;Miao&#x27;, &#x27;Zapata&#x27;, &#x27;Dahlstedt&#x27;, &#x27;Dahl&#x27;, &#x27;Bridgeman&#x27;, &#x27;Mc namara&#x27;, &#x27;Horah&#x27;, &#x27;Kerman&#x27;, &#x27;Poitras&#x27;, &#x27;Mayer&#x27;, &#x27;Badash&#x27;, &#x27;Findley&#x27;, &#x27;Galatioto&#x27;, &#x27;Ho-stuart&#x27;, &#x27;Marinko&#x27;, &#x27;Neuman&#x27;, &#x27;Marinko&#x27;, &#x27;Gottardo&#x27;, &#x27;Galatioto&#x27;, &#x27;Christofi&#x27;, &#x27;Mo&#x27;, &#x27;Dower&#x27;, &#x27;Buddha&#x27;, &#x27;Kerman&#x27;, &#x27;Drago&#x27;, &#x27;Horah&#x27;, &#x27;Po&#x27;, &#x27;Zapata&#x27;, &#x27;Galatioto&#x27;, &#x27;Kerman&#x27;, &#x27;Lemay&#x27;, &#x27;Schuler&#x27;, &#x27;Dower&#x27;, &#x27;Po&#x27;, &#x27;Butterfield&#x27;, &#x27;Neuman&#x27;, &#x27;Sicilia&#x27;, &#x27;Drago&#x27;, &#x27;Butterfield&#x27;, &#x27;Gottardo&#x27;, &#x27;Buddha&#x27;, &#x27;Dahl&#x27;, &#x27;Lowe&#x27;, &#x27;Hirsch&#x27;, &#x27;Sicilia&#x27;, &#x27;Berenguier&#x27;, &#x27;Kerman&#x27;, &#x27;Gottardo&#x27;, &#x27;Kerman&#x27;, &#x27;Dower&#x27;, &#x27;Fernandez cifuentes &#x27;, &#x27;Miao&#x27;, &#x27;Hatch&#x27;, &#x27;Schuler&#x27;, &#x27;Hirsch&#x27;, &#x27;Landry&#x27;, &#x27;Corfas&#x27;, &#x27;Rosenbloom&#x27;, &#x27;Lowe&#x27;, &#x27;Findley&#x27;, &#x27;Butterfield&#x27;, &#x27;Decaro&#x27;, &#x27;Coulson&#x27;, &#x27;Whitla&#x27;, &#x27;Miao&#x27;, &#x27;Coulson&#x27;, &#x27;Delossa&#x27;, &#x27;Gruber&#x27;, &#x27;Schuler&#x27;, &#x27;Neuman&#x27;, &#x27;Berenguier&#x27;, &#x27;Butterfield&#x27;, &#x27;Balster&#x27;, &#x27;Dahlstedt&#x27;, &#x27;Gruber&#x27;, &#x27;Gruber&#x27;, &#x27;Buddha&#x27;, &#x27;Suchoff&#x27;, &#x27;Ho-stuart&#x27;, &#x27;Gottardo&#x27;, &#x27;Whitla&#x27;, &#x27;Kerman&#x27;, &#x27;Corfas&#x27;, &#x27;Marinko&#x27;, &#x27;Cerda&#x27;, &#x27;Mc namara&#x27;, &#x27;Lowe&#x27;, &#x27;Whitla&#x27;, &#x27;Mignani&#x27;, &#x27;Schuler&#x27;, &#x27;Curlin&#x27;, &#x27;Dominguez&#x27;, &#x27;Neuman&#x27;, &#x27;Poitras&#x27;, &#x27;Dahlstedt&#x27;, &#x27;Mc namara&#x27;, &#x27;Egan&#x27;, &#x27;Balster&#x27;, &#x27;Balster&#x27;, &#x27;Zapata&#x27;, &#x27;Schuler&#x27;, &#x27;Hirsch&#x27;, &#x27;Poitras&#x27;, &#x27;Egan&#x27;, &#x27;Suchoff&#x27;, &#x27;Badash&#x27;, &#x27;Gottardo&#x27;, &#x27;Mayer&#x27;, &#x27;Cerda&#x27;, &#x27;Findley&#x27;, &#x27;Egan&#x27;, &#x27;Ardeni&#x27;, &#x27;Mo&#x27;, &#x27;Dower&#x27;, &#x27;Rosenbloom&#x27;, &#x27;Mayer&#x27;, &#x27;Macmahon&#x27;, &#x27;Srivastava&#x27;, &#x27;Delossa&#x27;, &#x27;Findley&#x27;, &#x27;Bridgeman&#x27;, &#x27;Zapata&#x27;, &#x27;Whitla&#x27;, &#x27;Suchoff&#x27;, &#x27;Rosenbloom&#x27;, &#x27;Hatch&#x27;, &#x27;Schuler&#x27;, &#x27;Dahlstedt&#x27;, &#x27;Cerda&#x27;, &#x27;Mo&#x27;, &#x27;Decaro&#x27;, &#x27;Coulson&#x27;, &#x27;Horah&#x27;, &#x27;Whitla&#x27;, &#x27;Rosenbloom&#x27;, &#x27;Gruber&#x27;, &#x27;Suchoff&#x27;, &#x27;Decaro&#x27;, &#x27;Findley&#x27;, &#x27;Egan&#x27;, &#x27;Srivastava&#x27;, &#x27;Whitla&#x27;, &#x27;Egan&#x27;, &#x27;Dominguez&#x27;, &#x27;Lemay&#x27;, &#x27;Dominguez&#x27;, &#x27;Whitla&#x27;, &#x27;Drago&#x27;, &#x27;Berenguier&#x27;, &#x27;Curlin&#x27;, &#x27;Cerda&#x27;, &#x27;Marinko&#x27;, &#x27;Poitras&#x27;, &#x27;Po&#x27;, &#x27;Findley&#x27;, &#x27;Butterfield&#x27;, &#x27;Macmahon&#x27;, &#x27;Dahl&#x27;, &#x27;Hirsch&#x27;, &#x27;Balster&#x27;, &#x27;Mc namara&#x27;, &#x27;Gottardo&#x27;, &#x27;Sicilia&#x27;, &#x27;Badash&#x27;, &#x27;Mayer&#x27;, &#x27;Dominguez&#x27;, &#x27;Marinko&#x27;, &#x27;Drago&#x27;, &#x27;Poitras&#x27;, &#x27;Sicilia&#x27;, &#x27;Cerda&#x27;, &#x27;Macmahon&#x27;, &#x27;Landry&#x27;, &#x27;Neuman&#x27;, &#x27;Decaro&#x27;, &#x27;Lemay&#x27;, &#x27;Hirsch&#x27;, &#x27;Delossa&#x27;, &#x27;Mignani&#x27;, &#x27;Mignani&#x27;, &#x27;Neuman&#x27;, &#x27;Rosenbloom&#x27;, &#x27;Rosenbloom&#x27;, &#x27;Miao&#x27;, &#x27;Zapata&#x27;, &#x27;Decaro&#x27;, &#x27;Landry&#x27;, &#x27;Cerda&#x27;, &#x27;Horah&#x27;, &#x27;Marinko&#x27;, &#x27;Findley&#x27;, &#x27;Dominguez&#x27;, &#x27;Coulson&#x27;, &#x27;Galatioto&#x27;, &#x27;Mignani&#x27;, &#x27;Drago&#x27;, &#x27;Mignani&#x27;, &#x27;Kerman&#x27;, &#x27;Rosenbloom&#x27;, &#x27;Hirsch&#x27;, &#x27;Lowe&#x27;, &#x27;Delossa&#x27;, &#x27;Fernandez cifuentes &#x27;, &#x27;Hirsch&#x27;, &#x27;Coulson&#x27;, &#x27;Corfas&#x27;, &#x27;Dahl&#x27;, &#x27;Poitras&#x27;, &#x27;Mc namara&#x27;, &#x27;Mo&#x27;, &#x27;Ho-stuart&#x27;, &#x27;Neuman&#x27;, &#x27;Drago&#x27;, &#x27;Lemay&#x27;, &#x27;Coulson&#x27;, &#x27;Egan&#x27;, &#x27;Landry&#x27;, &#x27;Berenguier&#x27;, &#x27;Coulson&#x27;, &#x27;Dower&#x27;, &#x27;Buddha&#x27;, &#x27;Marinko&#x27;, &#x27;Suchoff&#x27;, &#x27;Dahl&#x27;, &#x27;Ho-stuart&#x27;, &#x27;Buddha&#x27;, &#x27;Mignani&#x27;, &#x27;Bridgeman&#x27;, &#x27;Findley&#x27;, &#x27;Fernandez cifuentes &#x27;, &#x27;Dower&#x27;, &#x27;Ho-stuart&#x27;, &#x27;Curlin&#x27;, &#x27;Cerda&#x27;, &#x27;Ardeni&#x27;, &#x27;Coulson&#x27;, &#x27;Lowe&#x27;, &#x27;Zapata&#x27;, &#x27;Macmahon&#x27;, &#x27;Badash&#x27;, &#x27;Dominguez&#x27;, &#x27;Bridgeman&#x27;, &#x27;Berenguier&#x27;, &#x27;Ho-stuart&#x27;, &#x27;Dahlstedt&#x27;, &#x27;Bridgeman&#x27;, &#x27;Neuman&#x27;, &#x27;Hirsch&#x27;, &#x27;Sicilia&#x27;, &#x27;Gottardo&#x27;, &#x27;Srivastava&#x27;, &#x27;Hirsch&#x27;, &#x27;Hatch&#x27;, &#x27;Decaro&#x27;, &#x27;Coulson&#x27;, &#x27;Macmahon&#x27;, &#x27;Poitras&#x27;, &#x27;Cerda&#x27;, &#x27;Sicilia&#x27;, &#x27;Schuler&#x27;, &#x27;Hatch&#x27;, &#x27;Poitras&#x27;, &#x27;Mo&#x27;, &#x27;Whitla&#x27;, &#x27;Poitras&#x27;, &#x27;Curlin&#x27;, &#x27;Gruber&#x27;, &#x27;Coulson&#x27;, &#x27;Lemay&#x27;, &#x27;Butterfield&#x27;, &#x27;Neuman&#x27;, &#x27;Hirsch&#x27;, &#x27;Butterfield&#x27;, &#x27;Neuman&#x27;, &#x27;Neuman&#x27;, &#x27;Srivastava&#x27;, &#x27;Mo&#x27;, &#x27;Butterfield&#x27;, &#x27;Fernandez cifuentes &#x27;, &#x27;Macmahon&#x27;, &#x27;Sicilia&#x27;, &#x27;Kerman&#x27;, &#x27;Mo&#x27;, &#x27;Cerda&#x27;, &#x27;Hatch&#x27;, &#x27;Suchoff&#x27;, &#x27;Findley&#x27;, &#x27;Dominguez&#x27;, &#x27;Srivastava&#x27;, &#x27;Dahl&#x27;, &#x27;Findley&#x27;, &#x27;Dominguez&#x27;, &#x27;Ardeni&#x27;, &#x27;Hatch&#x27;, &#x27;Po&#x27;]</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "SELECT date, collect_list(customer.last_name) \n",
    "FROM orders \n",
    "GROUP BY date\n",
    "LIMIT 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KoJbUmwrQOmy"
   },
   "source": [
    "Now try it on yourself.\n",
    "\n",
    "For every order on 2016-1-1, return the list of products that appeared in that order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 90.6279296875,
      "end_time": 1620480224562.293
     }
    },
    "id": "GpWf6W_ZPg9-",
    "outputId": "59ca0ced-a3e0-46a2-a010-445e5efc372a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------------+\n",
      "|order_id|collect_list(i.product)|\n",
      "+--------+-----------------------+\n",
      "|      16|   [fan, whiskey bot...|\n",
      "|      93|              [toaster]|\n",
      "|     512|   [fan, headphones,...|\n",
      "|     772|   [fan, headphones,...|\n",
      "|     981|   [mouse trap, comp...|\n",
      "|    1176|   [stuffed animal, ...|\n",
      "|    1245|   [kettle, toaster,...|\n",
      "|    1280|   [whiskey bottle, ...|\n",
      "|    1590|   [fan, mouse trap,...|\n",
      "|    1799|   [toaster, noteboo...|\n",
      "|    1930|   [whiskey bottle, ...|\n",
      "|    2098|   [mouse trap, head...|\n",
      "|    2312|   [the book, toaste...|\n",
      "|    2434|   [the book, kettle...|\n",
      "|    2735|           [mouse trap]|\n",
      "|    2772|   [notebook, comput...|\n",
      "|    3120|   [whiskey bottle, ...|\n",
      "|    3166|    [notebook, toaster]|\n",
      "|    3217|   [headphones, head...|\n",
      "|    3315|   [stuffed animal, ...|\n",
      "+--------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "exploded_df = orders_df.select(explode(\"items\").alias(\"i\"), \"*\")\n",
    "exploded_df.filter(exploded_df[\"date\"] == \"2016-1-1\").groupBy(\"order_id\").agg(collect_list(\"i.product\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ifIhGcEQmrT"
   },
   "source": [
    "For every product, return the set of dates when it's purchased:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 996.149169921875,
      "end_time": 1620480623060.309
     }
    },
    "id": "ZEmGI5GUPg9-",
    "outputId": "dafebd33-ad91-4566-95cf-2942cbd25ba3",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+\n",
      "|        product|   collect_set(date)|\n",
      "+---------------+--------------------+\n",
      "|       the book|[2017-4-3, 2017-4...|\n",
      "|     mouse trap|[2017-4-3, 2017-4...|\n",
      "|computer screen|[2017-4-3, 2017-4...|\n",
      "| whiskey bottle|[2017-4-3, 2017-4...|\n",
      "|        toaster|[2017-4-3, 2017-4...|\n",
      "| stuffed animal|[2017-4-3, 2017-4...|\n",
      "|         kettle|[2017-4-3, 2017-4...|\n",
      "|            fan|[2017-4-3, 2017-4...|\n",
      "|     headphones|[2017-4-3, 2017-4...|\n",
      "|       notebook|[2017-4-3, 2017-4...|\n",
      "+---------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_set\n",
    "exploded_df.orderBy(\"date\").groupBy(\"i.product\").agg(collect_set(\"date\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uboC94LIRThc"
   },
   "source": [
    "One of the drawbacks of the <font face=\"courier\">collect_set/collect_list</font> method is they only accept one column as the argument. Later we will see how we can create nestedness on pretty much everything after we get the hang of the mighty JSONiq."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
